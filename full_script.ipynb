{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import asyncio\n",
    "import os\n",
    "import sqlite3\n",
    "from io import StringIO\n",
    "\n",
    "# Third-party imports\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from scrapling import StealthyFetcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrape one year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_year(player_id, year):\n",
    "    # The URL pattern for pitching game logs\n",
    "    url = f\"https://www.baseball-reference.com/players/gl.fcgi?id={player_id}&t=p&year={year}\"\n",
    "    page = await StealthyFetcher().async_fetch(url)  # the async version of fetch\n",
    "    soup = BeautifulSoup(page.html_content, 'html.parser')\n",
    "    #extract the player name from the title\n",
    "    player_name = ' '.join(soup.find('title').text.split()[:2]) if soup.find('title') else None\n",
    "    game_log_table = soup.find('table', id='pitching_gamelogs')\n",
    "    df = pd.read_html(StringIO(str(game_log_table)))[0] if game_log_table else []\n",
    "    if not isinstance(df, list) and not df.empty:\n",
    "        df['year'] = year\n",
    "        df['player_id'] = player_id\n",
    "        df['name'] = player_name\n",
    "    return df, player_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrape all years for 1 guy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def player_scrape(player_id):\n",
    "    \"\"\"Scrape and combine pitching data for 2021-2024 for a player.\"\"\"\n",
    "    years = [2021, 2022, 2023, 2024]  # Fixed years to scrape\n",
    "    tasks = [scrape_year(player_id, year) for year in years] # run all the years at once\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    # Combine dataframes, filtering out empty ones\n",
    "    dfs = [df for df, _ in results if not isinstance(df, list) and not df.empty]\n",
    "    combined_df = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse_pitcher_game_logs(df):\n",
    "    rename_map = {\n",
    "        'Rk': 'season_game_num',\n",
    "        'Gcar': 'career_game_num',\n",
    "        'Gtm': 'team_game_num',\n",
    "        'Date': 'date',\n",
    "        'Tm': 'team_id',\n",
    "        'Unnamed: 5': 'road_indicator',\n",
    "        'Opp': 'opp_id',\n",
    "        'Rslt': 'game_result',\n",
    "        'Inngs': 'innings',\n",
    "        'DR': 'days_rest',\n",
    "        'IP': 'ip',\n",
    "        'H': 'h',\n",
    "        'R': 'r',\n",
    "        'ER': 'er',\n",
    "        'BB': 'bb',\n",
    "        'SO': 'so',\n",
    "        'HR': 'hr',\n",
    "        'HBP': 'hbp',\n",
    "        'ERA': 'era',\n",
    "        'FIP': 'fip',\n",
    "        'BF': 'batters_faced',\n",
    "        'Pit': 'pitches',\n",
    "        'Str': 'strikes_total',\n",
    "        'StL': 'strikes_looking',\n",
    "        'StS': 'strikes_swinging',\n",
    "        'GB': 'ground_balls',\n",
    "        'FB': 'flyballs',\n",
    "        'LD': 'line_drives',\n",
    "        'PU': 'pop_ups',\n",
    "        'GSc': 'game_score',\n",
    "        'SB': 'sb',\n",
    "        'CS': 'cs',\n",
    "        'PO': 'pickoffs',\n",
    "        'AB': 'ab',\n",
    "        '2B': '2b',\n",
    "        '3B': '3b',\n",
    "        'IBB': 'ibb',\n",
    "        'GDP': 'gidp',\n",
    "        'SF': 'sf',\n",
    "        'ROE': 'roe',\n",
    "        'aLI': 'avg_leverage_index',\n",
    "        'WPA': 'win_prob_added',\n",
    "        'acLI': 'adjusted_cli_avg',\n",
    "        'cWPA': 'champ_win_prob_added',\n",
    "        'RE24': 'base_out_run_saved'\n",
    "    }\n",
    "    #drop un needed and apply column mapping\n",
    "    df = df.drop(columns=['Dec','IR','IS','Unk','DFS(DK)','DFS(FD)', 'Entered', 'Exited','Rslt','Inngs'], \n",
    "                                    errors='ignore').rename(columns=rename_map)\n",
    "    # Convert road_indicator to dummy variables and filter invalid team_id rows\n",
    "    df = pd.concat([df.drop('road_indicator', axis=1), \n",
    "                            pd.get_dummies(df['road_indicator'], prefix='road')], axis=1)\n",
    "    df = df[~((df['team_id'].isna()) | (df['team_id'] == \"Tm\"))]\n",
    "    # Ensure player_id, name, and year are first three columns\n",
    "    desired_order = ['player_id', 'name', 'year'] + [col for col in df.columns if col not in ['player_id', 'name', 'year']]\n",
    "    df = df[desired_order]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pitcher_logs(df, db_path=\"baseball.db\"):\n",
    "    db_path = os.path.abspath(db_path)\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(db_path) or '.', exist_ok=True)\n",
    "    # Create connection to SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    #save gamelogs\n",
    "    df.to_sql(\"pitching_gamelogs\", conn, if_exists='append', index=False)\n",
    "    \n",
    "    #save player info\n",
    "    player_info = df[['player_id', 'name']].drop_duplicates()\n",
    "    player_info.to_sql(\"scraped_players\", conn, if_exists='append', index=False)\n",
    "    \n",
    "    # Close connection\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"{df['name'].iloc[0]} finished\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put it all together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_all_players(filepath):\n",
    "    test_ids = (pd.read_csv(filepath))[\"player_id\"]\n",
    "    for player_id in test_ids:\n",
    "        print(player_id)\n",
    "        df = await player_scrape(player_id)\n",
    "        df = cleanse_pitcher_game_logs(df)\n",
    "        save_pitcher_logs(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run it on csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply nest_asyncio to allow asyncio to run in Jupyter Notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Filepath to your CSV file (update this if needed)\n",
    "csv_filepath = \"test_ids.csv\"  # Adjust if it's in a different directory\n",
    "\n",
    "# Run the async function in the event loop\n",
    "asyncio.run(scrape_all_players(csv_filepath))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get probable pitchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these imports if they are missing in your first cell\n",
    "# Standard library imports\n",
    "import asyncio\n",
    "import os\n",
    "from io import StringIO\n",
    "import traceback # For detailed error reporting\n",
    "\n",
    "# Third-party imports\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from scrapling import StealthyFetcher # Make sure this is imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_probable_pitchers(url=\"https://www.mlb.com/probable-pitchers\"):\n",
    "    \"\"\"\n",
    "    Scrapes probable pitcher names, opponents, and home/away status\n",
    "    from the MLB probable pitchers page using updated selectors based on HTML inspection.\n",
    "    Returns a list of dictionaries.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to fetch: {url}\")\n",
    "    results = []\n",
    "    try:\n",
    "        fetcher = StealthyFetcher()\n",
    "        page = await fetcher.async_fetch(url)\n",
    "\n",
    "        if not page or not page.html_content:\n",
    "             print(f\"Failed to fetch content from {url}. Status: {page.status_code if page else 'N/A'}\")\n",
    "             return []\n",
    "\n",
    "        print(\"Successfully fetched page content.\")\n",
    "        soup = BeautifulSoup(page.html_content, 'html.parser')\n",
    "\n",
    "        # Find each game matchup container\n",
    "        game_matchups = soup.select('div.probable-pitchers__matchup')\n",
    "\n",
    "        if not game_matchups:\n",
    "            print(\"Could not find game matchup containers using selector 'div.probable-pitchers__matchup'.\")\n",
    "            return []\n",
    "\n",
    "        print(f\"Found {len(game_matchups)} game matchups.\")\n",
    "        processed_count = 0\n",
    "\n",
    "        for i, game in enumerate(game_matchups):\n",
    "            try:\n",
    "                # --- Selectors Based on Provided HTML ---\n",
    "                away_team_element = game.select_one('span.probable-pitchers__team-name--away')\n",
    "                home_team_element = game.select_one('span.probable-pitchers__team-name--home')\n",
    "\n",
    "                # Select the links containing pitcher names within the pitcher section\n",
    "                pitcher_name_links = game.select('div.probable-pitchers__pitchers div.probable-pitchers__pitcher-name a')\n",
    "                # --- End Selectors ---\n",
    "\n",
    "                # --- Checks ---\n",
    "                missing = []\n",
    "                if not away_team_element: missing.append(\"Away Team\")\n",
    "                if not home_team_element: missing.append(\"Home Team\")\n",
    "                # Check if we found at least two pitcher links\n",
    "                if len(pitcher_name_links) < 2:\n",
    "                    if len(pitcher_name_links) == 1:\n",
    "                        missing.append(\"Second Pitcher\") # Found one, missing the other\n",
    "                    else:\n",
    "                        missing.append(\"Both Pitchers\") # Found none\n",
    "                # --- End Checks ---\n",
    "\n",
    "                if missing:\n",
    "                    print(f\"Skipping matchup {i+1}: Couldn't find -> {', '.join(missing)}\")\n",
    "                    continue\n",
    "\n",
    "                # --- Extract Data ---\n",
    "                away_team = away_team_element.get_text(strip=True)\n",
    "                home_team = home_team_element.get_text(strip=True)\n",
    "                # First link is away pitcher\n",
    "                pitcher1_name = pitcher_name_links[0].get_text(strip=True)\n",
    "                # Second link is home pitcher\n",
    "                pitcher2_name = pitcher_name_links[1].get_text(strip=True)\n",
    "                # --- End Extract Data ---\n",
    "\n",
    "                # Assign opponents and home/away status\n",
    "                # Pitcher 1 (Away)\n",
    "                if pitcher1_name and pitcher1_name != \"TBD\":\n",
    "                    results.append({'name': pitcher1_name, 'opponent': home_team, 'at_home': 0})\n",
    "\n",
    "                # Pitcher 2 (Home)\n",
    "                if pitcher2_name and pitcher2_name != \"TBD\":\n",
    "                     results.append({'name': pitcher2_name, 'opponent': away_team, 'at_home': 1})\n",
    "\n",
    "                processed_count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing matchup {i+1}: {e}\")\n",
    "                # traceback.print_exc() # Uncomment for deep debugging if needed\n",
    "                continue\n",
    "\n",
    "        print(f\"Finished processing matchups. Successfully extracted {len(results)} pitcher entries from {processed_count} fully processed matchups.\")\n",
    "        return results\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Required libraries (BeautifulSoup, scrapling) not found. Please install them using uv: `uv pip install beautifulsoup4 scrapling`\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An overall error occurred during scraping: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asyncio loop already running. Scheduling main_mlb task.\n",
      "\n",
      "MLB scraping process initiated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping MLB probable pitchers, opponents, and home/away status...\n",
      "Attempting to fetch: https://www.mlb.com/probable-pitchers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-08 11:14:07] INFO: Fetched (200) <GET https://www.mlb.com/probable-pitchers> (referer: https://www.google.com/search?q=mlb)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched page content.\n",
      "Found 15 game matchups.\n",
      "Skipping matchup 13: Couldn't find -> Second Pitcher\n",
      "Finished processing matchups. Successfully extracted 28 pitcher entries from 14 fully processed matchups.\n",
      "\n",
      "--- Probable Pitchers Found ---\n",
      "1. Carlos Carrasco (vs Tigers, Away)\n",
      "2. Tarik Skubal (vs Yankees, Home)\n",
      "3. Shane Smith (vs Guardians, Away)\n",
      "4. Ben Lively (vs White Sox, Home)\n",
      "5. Connor Gillispie (vs Mets, Away)\n",
      "6. Clay Holmes (vs Marlins, Home)\n",
      "7. Sonny Gray (vs Pirates, Away)\n",
      "8. Paul Skenes (vs Cardinals, Home)\n",
      "9. Easton Lucas (vs Red Sox, Away)\n",
      "10. Garrett Crochet (vs Blue Jays, Home)\n",
      "11. Justin Wrobleski (vs Nationals, Away)\n",
      "12. Brad Lord (vs Dodgers, Home)\n",
      "13. Kyle Hendricks (vs Rays, Away)\n",
      "14. Shane Baz (vs Angels, Home)\n",
      "15. Zack Wheeler (vs Braves, Away)\n",
      "16. Chris Sale (vs Phillies, Home)\n",
      "17. Pablo López (vs Royals, Away)\n",
      "18. Cole Ragans (vs Twins, Home)\n",
      "19. Patrick Corbin (vs Cubs, Away)\n",
      "20. Jameson Taillon (vs Rangers, Home)\n",
      "21. Freddy Peralta (vs Rockies, Away)\n",
      "22. Kyle Freeland (vs Brewers, Home)\n",
      "23. Charlie Morton (vs D-backs, Away)\n",
      "24. Merrill Kelly (vs Orioles, Home)\n",
      "25. Nick Lodolo (vs Giants, Away)\n",
      "26. Landen Roupp (vs Reds, Home)\n",
      "27. Dylan Cease (vs Athletics, Away)\n",
      "28. Jeffrey Springs (vs Padres, Home)\n",
      "\n",
      "Successfully saved list to probable_pitchers_with_opponents.csv\n"
     ]
    }
   ],
   "source": [
    "async def main_mlb():\n",
    "    print(\"\\nScraping MLB probable pitchers, opponents, and home/away status...\")\n",
    "    pitcher_data = await scrape_probable_pitchers() # Function now returns list of dicts with 'at_home'\n",
    "    if pitcher_data:\n",
    "        print(\"\\n--- Probable Pitchers Found ---\")\n",
    "        printed_names = set()\n",
    "        for i, data in enumerate(pitcher_data):\n",
    "             # Determine Home/Away string for printing\n",
    "             home_away_str = \"Home\" if data.get('at_home', 0) == 1 else \"Away\"\n",
    "             # Check for duplicates before printing\n",
    "             if data['name'] not in printed_names:\n",
    "                 print(f\"{i+1}. {data['name']} (vs {data['opponent']}, {home_away_str})\")\n",
    "                 printed_names.add(data['name'])\n",
    "             else:\n",
    "                  print(f\"   (Duplicate entry skipped: {data['name']})\")\n",
    "\n",
    "\n",
    "        # --- Save to CSV ---\n",
    "        try:\n",
    "            if 'pd' in globals() or 'pd' in locals():\n",
    "                df_pitchers = pd.DataFrame(pitcher_data)\n",
    "                 # Ensure columns exist even if list is empty\n",
    "                if 'name' not in df_pitchers.columns: df_pitchers['name'] = []\n",
    "                if 'opponent' not in df_pitchers.columns: df_pitchers['opponent'] = []\n",
    "                if 'at_home' not in df_pitchers.columns: df_pitchers['at_home'] = []\n",
    "\n",
    "\n",
    "                # Remove potential duplicates based on name before saving\n",
    "                df_pitchers = df_pitchers.drop_duplicates(subset=['name'], keep='first')\n",
    "\n",
    "                output_filename = 'probable_pitchers_with_opponents.csv' # Keep filename or change if preferred\n",
    "                # Specify column order including 'at_home'\n",
    "                df_pitchers.to_csv(output_filename, index=False, columns=['name', 'opponent', 'at_home'])\n",
    "                print(f\"\\nSuccessfully saved list to {output_filename}\")\n",
    "            else:\n",
    "                print(\"\\nWarning: pandas (pd) not imported. Cannot save to CSV.\")\n",
    "        except Exception as e:\n",
    "             print(f\"\\nError saving to CSV: {e}\")\n",
    "             traceback.print_exc()\n",
    "        # --------------------\n",
    "\n",
    "    else:\n",
    "        print(\"\\nNo probable pitchers found or an error occurred during scraping.\")\n",
    "\n",
    "# --- Run the main_mlb async function (code to run it remains the same) ---\n",
    "try:\n",
    "    loop = asyncio.get_running_loop()\n",
    "    print(\"Asyncio loop already running. Scheduling main_mlb task.\")\n",
    "    asyncio.ensure_future(main_mlb())\n",
    "except RuntimeError:\n",
    "    print(\"No asyncio loop running. Starting one with asyncio.run().\")\n",
    "    asyncio.run(main_mlb())\n",
    "\n",
    "print(\"\\nMLB scraping process initiated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

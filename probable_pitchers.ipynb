{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull pitchers from MLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these imports if they are missing in your first cell\n",
    "# Standard library imports\n",
    "import asyncio\n",
    "import os\n",
    "from io import StringIO\n",
    "import traceback # For detailed error reporting\n",
    "\n",
    "# Third-party imports\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from scrapling import StealthyFetcher # Make sure this is imported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Make sure re is imported, add 'import re' to your imports cell if needed\n",
    "\n",
    "async def scrape_probable_pitchers(url=\"https://www.mlb.com/probable-pitchers\"):\n",
    "    \"\"\"\n",
    "    Scrapes probable pitcher names, IDs, opponents, and home/away status\n",
    "    from the MLB probable pitchers page.\n",
    "    Returns a list of dictionaries.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to fetch: {url}\")\n",
    "    results = []\n",
    "    try:\n",
    "        fetcher = StealthyFetcher()\n",
    "        page = await fetcher.async_fetch(url)\n",
    "\n",
    "        if not page or not page.html_content:\n",
    "             print(f\"Failed to fetch content from {url}. Status: {page.status_code if page else 'N/A'}\")\n",
    "             return []\n",
    "\n",
    "        print(\"Successfully fetched page content.\")\n",
    "        soup = BeautifulSoup(page.html_content, 'html.parser')\n",
    "\n",
    "        game_matchups = soup.select('div.probable-pitchers__matchup')\n",
    "\n",
    "        if not game_matchups:\n",
    "            print(\"Could not find game matchup containers using selector 'div.probable-pitchers__matchup'.\")\n",
    "            return []\n",
    "\n",
    "        print(f\"Found {len(game_matchups)} game matchups.\")\n",
    "        processed_count = 0\n",
    "\n",
    "        for i, game in enumerate(game_matchups):\n",
    "            try:\n",
    "                away_team_element = game.select_one('span.probable-pitchers__team-name--away')\n",
    "                home_team_element = game.select_one('span.probable-pitchers__team-name--home')\n",
    "                pitcher_name_links = game.select('div.probable-pitchers__pitchers div.probable-pitchers__pitcher-name a')\n",
    "\n",
    "                missing = []\n",
    "                if not away_team_element: missing.append(\"Away Team\")\n",
    "                if not home_team_element: missing.append(\"Home Team\")\n",
    "                if len(pitcher_name_links) < 2:\n",
    "                    if len(pitcher_name_links) == 1: missing.append(\"Second Pitcher Link\")\n",
    "                    else: missing.append(\"Both Pitcher Links\")\n",
    "\n",
    "                if missing:\n",
    "                    print(f\"Skipping matchup {i+1}: Couldn't find -> {', '.join(missing)}\")\n",
    "                    continue\n",
    "\n",
    "                away_team = away_team_element.get_text(strip=True)\n",
    "                home_team = home_team_element.get_text(strip=True)\n",
    "\n",
    "                # --- Extract Pitcher 1 (Away) Info ---\n",
    "                pitcher1_link = pitcher_name_links[0]\n",
    "                pitcher1_name = pitcher1_link.get_text(strip=True)\n",
    "                pitcher1_href = pitcher1_link.get('href', '')\n",
    "                # Extract ID from href (e.g., /player/carlos-carrasco-471911 -> carlos-carrasco-471911)\n",
    "                pitcher1_id_match = re.search(r'/player/([^/]+)$', pitcher1_href)\n",
    "                pitcher1_id = pitcher1_id_match.group(1) if pitcher1_id_match else None\n",
    "\n",
    "                # --- Extract Pitcher 2 (Home) Info ---\n",
    "                pitcher2_link = pitcher_name_links[1]\n",
    "                pitcher2_name = pitcher2_link.get_text(strip=True)\n",
    "                pitcher2_href = pitcher2_link.get('href', '')\n",
    "                pitcher2_id_match = re.search(r'/player/([^/]+)$', pitcher2_href)\n",
    "                pitcher2_id = pitcher2_id_match.group(1) if pitcher2_id_match else None\n",
    "\n",
    "\n",
    "                # Assign opponents and home/away status\n",
    "                if pitcher1_name and pitcher1_name != \"TBD\" and pitcher1_id:\n",
    "                    results.append({\n",
    "                        'name': pitcher1_name,\n",
    "                        'player_id': pitcher1_id, # Add player ID\n",
    "                        'opponent': home_team,\n",
    "                        'at_home': 0\n",
    "                    })\n",
    "\n",
    "                if pitcher2_name and pitcher2_name != \"TBD\" and pitcher2_id:\n",
    "                     results.append({\n",
    "                         'name': pitcher2_name,\n",
    "                         'player_id': pitcher2_id, # Add player ID\n",
    "                         'opponent': away_team,\n",
    "                         'at_home': 1\n",
    "                     })\n",
    "\n",
    "                processed_count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing matchup {i+1}: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"Finished processing matchups. Successfully extracted {len(results)} pitcher entries from {processed_count} fully processed matchups.\")\n",
    "        return results\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Required libraries (BeautifulSoup, scrapling, re) not found.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An overall error occurred during scraping: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asyncio loop already running. Scheduling main_mlb task.\n",
      "\n",
      "MLB scraping process initiated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping MLB probable pitchers, IDs, opponents, and home/away status...\n",
      "Attempting to fetch: https://www.mlb.com/probable-pitchers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-08 11:21:23] INFO: Fetched (200) <GET https://www.mlb.com/probable-pitchers> (referer: https://www.google.com/search?q=mlb)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched page content.\n",
      "Found 15 game matchups.\n",
      "Skipping matchup 13: Couldn't find -> Second Pitcher Link\n",
      "Finished processing matchups. Successfully extracted 28 pitcher entries from 14 fully processed matchups.\n",
      "\n",
      "--- Probable Pitchers Found ---\n",
      "1. Carlos Carrasco (carlos-carrasco-471911) (vs Tigers, Away)\n",
      "2. Tarik Skubal (tarik-skubal-669373) (vs Yankees, Home)\n",
      "3. Shane Smith (shane-smith-681343) (vs Guardians, Away)\n",
      "4. Ben Lively (ben-lively-594902) (vs White Sox, Home)\n",
      "5. Connor Gillispie (connor-gillispie-687362) (vs Mets, Away)\n",
      "6. Clay Holmes (clay-holmes-605280) (vs Marlins, Home)\n",
      "7. Sonny Gray (sonny-gray-543243) (vs Pirates, Away)\n",
      "8. Paul Skenes (paul-skenes-694973) (vs Cardinals, Home)\n",
      "9. Easton Lucas (easton-lucas-687922) (vs Red Sox, Away)\n",
      "10. Garrett Crochet (garrett-crochet-676979) (vs Blue Jays, Home)\n",
      "11. Justin Wrobleski (justin-wrobleski-680736) (vs Nationals, Away)\n",
      "12. Brad Lord (brad-lord-695418) (vs Dodgers, Home)\n",
      "13. Kyle Hendricks (kyle-hendricks-543294) (vs Rays, Away)\n",
      "14. Shane Baz (shane-baz-669358) (vs Angels, Home)\n",
      "15. Zack Wheeler (zack-wheeler-554430) (vs Braves, Away)\n",
      "16. Chris Sale (chris-sale-519242) (vs Phillies, Home)\n",
      "17. Pablo LÃ³pez (pablo-lopez-641154) (vs Royals, Away)\n",
      "18. Cole Ragans (cole-ragans-666142) (vs Twins, Home)\n",
      "19. Patrick Corbin (patrick-corbin-571578) (vs Cubs, Away)\n",
      "20. Jameson Taillon (jameson-taillon-592791) (vs Rangers, Home)\n",
      "21. Freddy Peralta (freddy-peralta-642547) (vs Rockies, Away)\n",
      "22. Kyle Freeland (kyle-freeland-607536) (vs Brewers, Home)\n",
      "23. Charlie Morton (charlie-morton-450203) (vs D-backs, Away)\n",
      "24. Merrill Kelly (merrill-kelly-518876) (vs Orioles, Home)\n",
      "25. Nick Lodolo (nick-lodolo-666157) (vs Giants, Away)\n",
      "26. Landen Roupp (landen-roupp-694738) (vs Reds, Home)\n",
      "27. Dylan Cease (dylan-cease-656302) (vs Athletics, Away)\n",
      "28. Jeffrey Springs (jeffrey-springs-605488) (vs Padres, Home)\n",
      "\n",
      "Successfully saved list to probable_pitchers_with_opponents.csv\n"
     ]
    }
   ],
   "source": [
    "async def main_mlb():\n",
    "    print(\"\\nScraping MLB probable pitchers, IDs, opponents, and home/away status...\")\n",
    "    pitcher_data = await scrape_probable_pitchers() # Function now returns list of dicts with player_id\n",
    "    if pitcher_data:\n",
    "        print(\"\\n--- Probable Pitchers Found ---\")\n",
    "        printed_names = set()\n",
    "        for i, data in enumerate(pitcher_data):\n",
    "             home_away_str = \"Home\" if data.get('at_home', 0) == 1 else \"Away\"\n",
    "             player_id_str = data.get('player_id', 'N/A') # Get player ID, default to N/A if missing\n",
    "\n",
    "             if data['name'] not in printed_names:\n",
    "                 # Include player ID in the printout\n",
    "                 print(f\"{i+1}. {data['name']} ({player_id_str}) (vs {data['opponent']}, {home_away_str})\")\n",
    "                 printed_names.add(data['name'])\n",
    "             else:\n",
    "                  print(f\"   (Duplicate entry skipped: {data['name']})\")\n",
    "\n",
    "\n",
    "        # --- Save to CSV ---\n",
    "        try:\n",
    "            if 'pd' in globals() or 'pd' in locals():\n",
    "                df_pitchers = pd.DataFrame(pitcher_data)\n",
    "\n",
    "                # Ensure columns exist\n",
    "                expected_cols = ['name', 'player_id', 'opponent', 'at_home']\n",
    "                for col in expected_cols:\n",
    "                     if col not in df_pitchers.columns: df_pitchers[col] = []\n",
    "\n",
    "\n",
    "                # Remove duplicates based on name\n",
    "                df_pitchers = df_pitchers.drop_duplicates(subset=['name'], keep='first')\n",
    "\n",
    "                output_filename = 'probable_pitchers_with_opponents.csv'\n",
    "                # Specify column order including 'player_id'\n",
    "                df_pitchers.to_csv(output_filename, index=False, columns=expected_cols)\n",
    "                print(f\"\\nSuccessfully saved list to {output_filename}\")\n",
    "            else:\n",
    "                print(\"\\nWarning: pandas (pd) not imported. Cannot save to CSV.\")\n",
    "        except Exception as e:\n",
    "             print(f\"\\nError saving to CSV: {e}\")\n",
    "             traceback.print_exc()\n",
    "        # --------------------\n",
    "\n",
    "    else:\n",
    "        print(\"\\nNo probable pitchers found or an error occurred during scraping.\")\n",
    "\n",
    "# --- Run the main_mlb async function (code to run it remains the same) ---\n",
    "try:\n",
    "    loop = asyncio.get_running_loop()\n",
    "    print(\"Asyncio loop already running. Scheduling main_mlb task.\")\n",
    "    asyncio.ensure_future(main_mlb())\n",
    "except RuntimeError:\n",
    "    print(\"No asyncio loop running. Starting one with asyncio.run().\")\n",
    "    asyncio.run(main_mlb())\n",
    "\n",
    "print(\"\\nMLB scraping process initiated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
